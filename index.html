<!DOCTYPE html>
<html lang='en'>

<head>
    <meta charset='UTF-8' />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="images/headshot.jpeg">
    <title>Ben Agro</title>
    <!-- Google fonts -->
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap');
    </style>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
        rel="stylesheet">
    <!-- github icon -->
    <!--link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"-->
    <script src="https://kit.fontawesome.com/6b8f993848.js" crossorigin="anonymous"></script>
    <!-- custom stylesheet -->
    <link rel='stylesheet' href='styles.css' />


</head>

<body>
    <div class="nav-div">
        <div class="nav-background">
            <nav class="nav-bar">
                <ul>
                    <li><a href='#sec-about'>Home</a></li>
                    <li><a href='#sec-experience'>Experience</a></li>
                    <li><a href='#sec-publications'>Publications</a></li>
                    <li><a href='#sec-projects'>Personal Projects</a></li>
                    <li><a href='#sec-climbing'>Climbing</a></li>
                    <li><a href='#sec-news'>News</a></li>
                    <li><a href='#sec-blog'>Blog</a></li>
                </ul>
            </nav>
        </div>
    </div>
    <div class="page">
        <!--nav class="nav-sidebar filler"></nav-->
        <section id="sec-about">
            <figure class="headshot-figure">
                <img src='images/headshot.jpeg' alt='Picture of Ben Agro' class="headshot" />
            </figure>
            <h1 class="my-name">Ben Agro</h1>
            <ul class="icons">
                <li><a href="https://github.com/BenAgro314" class="fa fa-github"></a></li>
                <li><a href="https://www.linkedin.com/in/ben-agro-43a619196/" class="fa fa-linkedin"></a></li>
                <li><a href="mailto: ben.agro@mail.utoronto.ca" class="fa fa-envelope"></a></li>
                <li><a href="files/ben_agro_resume.pdf" class="fa fa-file"></a></li>
            </ul>
            <p>
                Welcome to my website!
                My interests are real-world robotics and understanding (machine) intelligence.
                I have been a high-level rock climber for the majority of my life.
            </p>
        </section>
        <section id="sec-experience">
            <h2>Experience</h2>
            <div class="row-experience">
                <div class="timespan">
                    <div>
                        <a href="https://waabi.ai/">
                            <img src="images/Waabi.svg" alt="Waabi Logo" class="logo-img" />
                        </a>
                        <time datetime='2022-5'>May 2022 - Present</time>
                    </div>
                </div>
                <div class="experience-desc">
                    I am a researcher at <a href="https://waabi.ai/">Waabi</a> working on next-generation autonomy
                    systems under <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a> (who is also my PhD supervisor).
                </div>
            </div>
            <div class="row-experience">
                <div class="timespan">
                    <div>
                        <a href="https://rvl.cs.toronto.edu/#/">
                            <img src="images/RVL.png" alt="RVL Logo" class="logo-img" />
                        </a>
                        <time datetime='2021'>Summer 2021</time>
                    </div>
                </div>
                <div class="experience-desc">
                    I was a research intern at the <a href="https://rvl.cs.toronto.edu/#/">Robotics Vision and Learning
                        Lab</a> supervised by <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>.
                    We were working on learning methods for task and motion planning, and I developed a new algorithm
                    for <a href="https://github.com/caelan/pddlstream">PDDLStream</a> that learned-task specific
                    heuristics for expanding the space of possible robot actions.
                </div>
            </div>
            <div class="row-experience">
                <div class="timespan">
                    <div>
                        <a href="http://asrl.utias.utoronto.ca/">
                            <img src="images/ASRL.png" alt="ASRL Logo" class="logo-img" />
                        </a>
                        <time datetime='2020'>Summer 2020</time>
                    </div>
                </div>
                <div class="experience-desc">
                    I was a research intern at the <a href="http://asrl.utias.utoronto.ca/">Autonomous Space and
                        Robotics Lab</a> supervised by <a href="http://asrl.utias.utoronto.ca/~tdb/">Tim Barfoot</a>.
                    We worked on self-supervised semantic LiDAR segmentation for autonomous navigation. I developed a
                    simulation of a complex indoor environment complete with dynamic actors and an
                    augmented navigation stack used to train and evaluate our method.
                </div>
            </div>
        </section>
        <section id="sec-publications">
            <!--h2><a href="https://scholar.google.ca/citations?user=FzHJoMcAAAAJ&hl=en">Publications</a></h2-->
            <h2>Publications</h2>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <div class="pub-img">
                            <img src="images/detra_teaser.gif" alt="A gif showing DeTras detections and forecasts"
                                style="width: 350%; object-fit: contain;" />
                        </div>
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">DeTra: A Unified Model for Object Detection and Trajectory Forecasting</h3>
                            <ul>
                                <li><a href="https://waabi.ai/detra/" class="fa-solid fa-link"></a></li>
                                <li><a href="https://arxiv.org/abs/2406.04426"
                                        class="fa-solid fa-file-pdf"></a></li>
                            </ul>
                            <h4 class="paper-authors">
                                Sergio Casas<sup>*</sup>,
                                <strong>Ben Agro</strong><sup>*</sup>, 
                                Jiageng Mao<sup>*</sup>,
                                Thomas Gilles,
                                Alexander Cui,
                                Thomas Li,
                                Raquel Urtasun</h4>
                            <!--h4 class="paper-journal">CVPR 2024 <b>(Oral, top 0.7%)</b></h4-->
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    Previous works perform object detection and trajectory forecasting through cascading modules. We reformulate
                    this as a single unified <i>trajectory refinement</i>  task, which removes the problem of compounding errors.
                    To fulfill this task, we propose a flexible transformer refinement archiecture that is easily extensible to
                    alternative input modalities and tasks.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <!--img src="images/panda-pic.png" alt="Picture of a Franka Panda stacking blocks"
                            style="width: 100%; height: 135px; object-fit: cover;" /-->
                        <!--img src="images/uno_logo.png" alt="UnO Logo"
                            style="width: 100%; height: 200px; object-fit: contain;" /-->
                        <video src="images/occ_field.mp4" autoplay loop muted style="width: 100%; height: 200px; object-fit: cover;">
                            A video of UnO's occupancy forecasts.
                        </video>
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">UnO: Unsupervised Occupancy Fields for Perception and Forecasting</h3>
                            <!--ul>
                                <li><a href="https://waabi.ai/research/" class="fa-solid fa-link"></a></li>
                                <li><a href="https://waabi.ai/wp-content/uploads/2023/05/ImplicitO-paper.pdf"
                                        class="fa-solid fa-file-pdf"></a></li>
                            </ul-->
                            <h4 class="paper-authors"><strong>Ben
                                    Agro</strong><sup>*</sup>, Quinlan Sykora<sup>*</sup>,
                                Sergio Casas,
                                Thomas Gilles,
                                Raquel Urtasun</h4>
                            <h4 class="paper-journal">CVPR 2024 <b>(Oral, top 0.7%)</b></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    We introduce a foundation model for the physical world that learns to perceive and forecast
                    4D (spatio-temporal) occupancy fields with self-supervision from LiDAR data. We show
                    that this model can be transfered to a variety of downstream tasks, such as LiDAR forecasting
                    and semantic birds eye view occupancy forecasting.
                    We came first in the <a href="https://eval.ai/web/challenges/challenge-page/1977/leaderboard/4662">CVPR 2024 Argoverse 2 LiDAR 4D Occupancy Challenge</a>.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/dumbgen_v2.png" alt="A cartoon describing certifiable optimization"
                            style="width: 100%; height: 200px; object-fit: contain;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Toward Globally Optimal State Estimation Using Automatically
                                Tightened Semidefinite Relaxations</h3>
                            <ul>
                                <li><a href="https://arxiv.org/pdf/2308.05783.pdf" class="fa-solid fa-file-pdf"></a>
                                </li>
                            </ul>
                            <h4 class="paper-authors">Frederike Dümbgen, Connor Holmes, <strong>Ben Agro</strong>,
                                Timothy D. Barfoot.</h4>
                            <h4 class="paper-journal">Pre-print</h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    During my <a href="files/bagro_engsci_thesis.pdf">undergraduate thesis</a>, I spent much time
                    trying to find redundant constraints to <q>tighten</q> an optimization problem
                    such that it was globally optimal.
                    Finding these redundant constraints was time-consuming and tedious.
                    This paper presents an automated method for finding redundant constraints for optimization problems.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <!--img src="images/panda-pic.png" alt="Picture of a Franka Panda stacking blocks"
                            style="width: 100%; height: 135px; object-fit: cover;" /-->
                        <video height="130px" autoplay="autoplay" loop muted>
                            <source src="images/ImplicitO_teaser_resized.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Implicit Occupancy Flow Fields for Perception and Prediction in
                                Self-Driving</h3>
                            <ul>
                                <li><a href="https://waabi.ai/research/" class="fa-solid fa-link"></a></li>
                                <li><a href="https://waabi.ai/wp-content/uploads/2023/05/ImplicitO-paper.pdf"
                                        class="fa-solid fa-file-pdf"></a></li>
                            </ul>
                            <h4 class="paper-authors"><strong>Ben
                                    Agro</strong><sup>*</sup>, Quinlan Sykora<sup>*</sup>,
                                Sergio Casas<sup>*</sup>,
                                Raquel Urtasun</h4>
                            <h4 class="paper-journal">CVPR 2023 <b>(Highlight)</b></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    A new approach to perception and motion-forcasting for self-driving vehicles using
                    a neural network to implicitly represent future occupancy and flow directly
                    from sensor data.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/panda-pic.png" alt="Picture of a Franka Panda stacking blocks"
                            style="width: 100%; height: 135px; object-fit: cover;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Learning to Search in Task and Motion Planning with Streams</h3>
                            <ul>
                                <li><a href="https://rvl.cs.toronto.edu/learning-based-tamp/"
                                        class="fa-solid fa-link"></a></li>
                                <li><a href="https://arxiv.org/abs/2111.13144" class="fa-solid fa-file-pdf"></a></li>
                                <li><a href="https://github.com/rvl-lab-utoronto/drake-tamp" class="fa fa-github"></a>
                                </li>
                                <li><a href="https://www.youtube.com/watch?v=pzzpR4wh_Zk"
                                        class="fa-brands fa-youtube"></a></li>
                            </ul>
                            <h4 class="paper-authors">Mohammed Khodeir<sup>*</sup>, <strong>Ben
                                    Agro</strong><sup>*</sup>, Florian Shkurti</h4>
                            <h4 class="paper-journal">CoRR 2021</h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    Presents a new algorithm for <a href="https://github.com/caelan/pddlstream">PDDLStream</a> that uses
                    a graph neural network to search for geometrically feasible plans in a "best first" manner.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/self-supervised-segmentation-asrl.jpg"
                            alt="Diagram of thet semantic segmentation pipeline" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor
                                Navigation</h3>
                            <ul>
                                <li><a href="https://arxiv.org/abs/2012.05897" class="fa-solid fa-file-pdf"></a></li>
                                <li><a href="https://github.com/utiasASRL/Crystal_Ball_Nav" class="fa fa-github"></a>
                                </li>
                                <li><a href="https://www.youtube.com/watch?v=Yyyrbnjqngg"
                                        class="fa-brands fa-youtube"></a></li>
                            </ul>
                            <h4 class="paper-authors">Hugues Thomas, <strong>Ben Agro</strong>, Mona Gridseth, Jian
                                Zhang, Timothy D. Barfoot</h4>
                            <h4 class="paper-journal">ICRA 2021</h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    A self-supervised learning approach for semantic segmentation of LiDAR over repeated navigation
                    sessions.
                </div>
            </div>
        </section>

        <!--section id="sec-blog">
            <h2>Blog</h2>
            <ul class="spaced-list">
                <li><b>May 2023</b>:<a href="blogs/engsci_review.html"> A review of UofT Engineering
                        Science</a></li>
            </ul>
        </section-->
        <section id="sec-projects">
            <h2>Personal Projects</h2>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/BARFT_thumbnail.png" alt="BARFT Thumbnail"
                            style="width: 100%;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">BARFT: Bundle Adjusting Neural Radiance Fields with Temporal Regularization</h3>
                            <ul>
                                <!--li><a href="https://rvl.cs.toronto.edu/learning-based-tamp/"
                                        class="fa-solid fa-link"></a></li-->
                                <li><a href="files/CSC2532_Final_Project_Report.pdf" class="fa-solid fa-file-pdf"></a></li>
                                <li><a href="https://github.com/souravb111/bundle-adjusting-NeRF/tree/bagro/sched"
                                        class="fa fa-github"></a>
                                </li>
                            </ul>
                            <h4 class="paper-journal"><time datetime="2023">2024</time></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    A framework for training NeRFs with unknown (learned) camera poses.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/TaS_thumbnail.png" alt="TaS Thumbnail"
                            style="width: 100%;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">An explainer of ``Transformers As Statisticans"</h3>
                            <ul>
                                <!--li><a href="https://rvl.cs.toronto.edu/learning-based-tamp/"
                                        class="fa-solid fa-link"></a></li-->
                                <li><a href="files/TaS_report.pdf" class="fa-solid fa-file-pdf"></a></li>
                            </ul>
                            <h4 class="paper-journal"><time datetime="2023">2024</time></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    This report distills the key ideas behind the paper <a href="https://arxiv.org/abs/2306.04637">Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection</a>.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/CSC2508_hook.png" alt="TaS Thumbnail"
                            style="width: 100%;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Zero-Shot Video Retrieval with Vision Language Models</h3>
                            <ul>
                                <li><a href="https://github.com/BenAgro314/CSC2508_final_project"
                                        class="fa-solid fa-link"></a></li>
                                <li><a href="files/agrobenj_CSC2508_Final_Report.pdf" class="fa-solid fa-file-pdf"></a></li>
                            </ul>
                            <h4 class="paper-journal"><time datetime="2023">2024</time></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    A zero-shot video retrieval system leveraging open-source Vision-Language models.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/thesis_thumbnail.png" alt="Stereo Localization Thumbnail"
                            style="width: 100%; height: 135px; object-fit: cover;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Towards Globally Optimal Stereo Localization (Undergrad Thesis)</h3>
                            <ul>
                                <!--li><a href="https://rvl.cs.toronto.edu/learning-based-tamp/"
                                        class="fa-solid fa-link"></a></li-->
                                <li><a href="files/bagro_engsci_thesis.pdf" class="fa-solid fa-file-pdf"></a></li>
                                <li><a href="https://github.com/BenAgro314/bagro_engsci_thesis"
                                        class="fa fa-github"></a>
                                </li>
                                <li><a href="https://www.youtube.com/watch?v=_fX9ZbzrwBE"
                                        class="fa-brands fa-youtube"></a></li>
                            </ul>
                            <h4 class="paper-journal"><time datetime="2023">2023</time></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    This is my undergrad thesis for Engineering Science at UofT under Prof. <a
                        href="http://asrl.utias.utoronto.ca/~tdb/">Tim Barfoot</a>.
                    We investigated how to make the problem of stereo localization (determining
                    the pose of a stereo camera with respect to observed landmarks) globally optimal.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/ct4.gif" alt="Video of Captor on CT4"
                            style="width: 100%; height: 135px; object-fit: cover;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Captor (Autonomous Drone)</h3>
                            <ul>
                                <!--li><a href="https://rvl.cs.toronto.edu/learning-based-tamp/"
                                        class="fa-solid fa-link"></a></li-->
                                <li><a href="files/Captor_Report.pdf" class="fa-solid fa-file-pdf"></a></li>
                                <li><a href="https://github.com/BenAgro314/rob498_capstone" class="fa fa-github"></a>
                                </li>
                                <li><a href="https://youtu.be/bnN9CyDADcY" class="fa-brands fa-youtube"></a></li>
                            </ul>
                            <!--h4 class="paper-authors">Mohammed Khodeir<sup>*</sup>, <strong>Ben
                                    Agro</strong><sup>*</sup>, Florian Shkurti</h4-->
                            <h4 class="paper-journal"><time datetime="2023">2023</time></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    I built and programmed an autonomous drone with reliable onboard SLAM and vision-only obstacle
                    avoidance.
                    The simulator I built for this project is <a
                        href="https://github.com/BenAgro314/catkin_ws_engsci">here</a>.
                </div>
            </div>
            <div class="row-publication">
                <div class="publication-header">
                    <div class="pub-img">
                        <img src="images/geo_boy_short.gif" alt="Video of Geometry Boy"
                            style="width: 100%; height: 135px; object-fit: cover;" />
                    </div>
                    <div class="pub-meta">
                        <div>
                            <h3 class="paper-title">Geometry Boy</h3>
                            <ul>
                                <!--li><a href="https://rvl.cs.toronto.edu/learning-based-tamp/"
                                        class="fa-solid fa-link"></a></li-->
                                <li><a href="files/Geometry_Boy_Report.pdf" class="fa-solid fa-file-pdf"></a></li>
                                <li><a href="https://github.com/BenAgro314/MIE438_Project" class="fa fa-github"></a>
                                </li>
                                <li><a href="https://youtu.be/1WBaEiE1EUo" class="fa-brands fa-youtube"></a></li>
                            </ul>
                            <!--h4 class="paper-authors">Mohammed Khodeir<sup>*</sup>, <strong>Ben
                                    Agro</strong><sup>*</sup>, Florian Shkurti</h4-->
                            <h4 class="paper-journal"><time datetime="2022">2022</time></h4>
                        </div>
                    </div>
                </div>
                <div class="publication-summary">
                    I programmed a version of the popular game <a href="https://geometrydash.io/">Geometry Dash</a>
                    that runs on the original <a href="https://en.wikipedia.org/wiki/Game_Boy">Gameboy</a> hardware.
                </div>
            </div>
        </section>

        <section id="sec-climbing">
            <h2>Climbing</h2>
            <!--i class="fa-solid fa-person-digging"></i-->
            <!--div>Under Construction</div-->
            <p style="padding-bottom: 15px;">I love bouldering, and my focus is on sending hard boulders outdoors. My
                current goal is to send <s>V13</s> V14.
                Here is some of my climbing related media:
            </p>

            <ul class="spaced-list">
                <li><b>Red Rock (Winter 2023)</b>
                    <a href="https://www.youtube.com/watch?v=dlhQ3oO69IU">Video</a> from my first trip to Red Rock.
                </li>
                <li><b>Squamish (Summer 2022)</b>: <a href="https://www.youtube.com/watch?v=RCzC7o2hwVw&t=6s">Video</a>
                    from my first real outdoor climbing trip. Highlights include sending The Summoning (V12) and Room
                    Service (V12).</li>
                <li><b>Climbing Training</b>: <a
                        href="https://www.youtube.com/playlist?list=PLWPCgxnTLbvFOwuky4TcwKlXCHBnIc4AR">Playlist of
                        videos</a> (mainly board climbing) from my climbing training.</li>
                <li><b>Sendage</b>: <a href="https://sendage.com/user/benagro">List</a> of my outdoor sends.</li>
                <li><b>Instagram</b>: My <a href="https://www.instagram.com/ben_agro/">Instagram</a> has lots of older
                    videos of me climbing.</li>
            </ul>

        </section>

        <section id="sec-news">
            <h2>News</h2>
            <ul class="spaced-list">
                <li><b>June 20, 2023</b>: I'll be at CVPR2024 to present UnO!</li>
                <li><b>June 20, 2023</b>: I'll be at CVPR2023 to present <a
                        href="https://waabi.ai/implicito/">ImplicitO</a>!
                </li>
                <li><b>June 12, 2023</b>: <a
                        href="https://news.engineering.utoronto.ca/top-u-of-t-undergraduate-ben-agro-is-taking-his-passion-for-research-into-a-direct-entry-phd/">
                        Top U of T undergraduate Ben Agro is taking his passion for research into a direct-entry
                        PhD</a>.</li>
            </ul>

        </section>

        <section id="sec-blog">
            <h2>Blog</h2>
            <ul class="spaced-list">
                <li><time datetime="2024-01-13"><b>Jan 13, 2023</b>:<a href="blogs/lessons_from_2nd_climbing_trip.html"> 
                        Lessons From My Second Outdoor Climbing Trip</a></li>
                <li><time datetime="2024-01-20"><b>Jan 13, 2023</b>:<a href="blogs/succeeding_in_engsci.html"> 
                        "Succeeding" in EngSci</a></li>
            </ul>
            <!--div class="under-construction">
                <i class="fa-solid fa-person-digging">
                </i>
                <p>Under Construction</p>
            </div-->
        </section>
    </div>
    <footer class="website-footer">
        <div class="footer-content">
            <div class="author">By Ben Agro, <time datetime="2024">2024</time></div>
            <div><a href="#sec-about" class="fa fa-arrow-up"></a></div>
        </div>
    </footer>
</body>

</html>